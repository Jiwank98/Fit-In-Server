# -*- coding: utf-8 -*-
"""DDQN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lai3yS_e9WU5ko4t7f-6qtn6NqX5j4s8
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.functional import mse_loss
import collections
import matplotlib
import matplotlib.pyplot as plt
import random
import math
from torch.autograd import Variable
import glob
import os
import torch.nn.functional as F
import copy
from tqdm import tqdm
import datetime
import scipy.io as sio
import pickle

save_time = datetime.datetime.today().strftime("%y%m%d_%H%M%S")
print(f'Save file name: {save_time}')

mat_path = './results/'
mat_name = f'run_{save_time}.mat'


def createFolder(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print('Error: Creating directory. ' + directory)


def saveMAT(self, mat_path, mat_name):
    createFolder(mat_path)
    sio.savemat(mat_path + mat_name, self.output)

save_time = datetime.datetime.today().strftime("%y%m%d_%H%M%S")
print(f'Save file name: {save_time}')


class Config(object):
    lr = 0.0001
    discount_factor = 0.99
    reply_buffer_limit = 50000
    update_target_frequency = 20
    initial_epsilon = 0.9
    min_epsilon = 0.0001
    epsilon_discount_rate = 1e-7
    batch_size = 32
    epochs= 50000

class Model(nn.Module):

    def __init__(self, action_num):
        super().__init__()
        self.rf_dim = 5 *4
        self.cf_dim = 4
        self.action_dim = 1
        self.input_dim = self.rf_dim + self.cf_dim + self.action_dim

        self.fc1 = nn.Linear(self.input_dim, 64)
        self.fc2 = nn.Linear(64,64)
        self.fc3 = nn.Linear(64,64)
        self.fc4 = nn.Linear(64, 1)
        self.relu = torch.nn.ReLU()


    def forward(self, state, action):
        state = torch.Tensor(state)
        inputs = torch.cat([state, action],dim=-1)
        x = self.relu(self.fc1(inputs))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.fc4(x)

        return x


from collections import deque
import random
import numpy as np

class Replay_Buffer:
    def __init__(self, buffer_limit):
        self.buffer = deque()
        self.buffer_limit = buffer_limit

    def size(self):
        return len(self.buffer)


    def append(self, transition):
        if len(self.buffer) > self.buffer_limit:
            self.buffer.popleft()
        self.buffer.append(transition)


    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        batch_rf, batch_cf, batch_action, batch_reward, batch_rf_new, batch_cf_new, batch_action_new = [], [], [], [], [], [], []

        for transition in batch:
            rf,cf,a,r,rf_prime, cf_prime, action_prime = transition
            batch_rf.append(rf)
            batch_cf.append(cf)
            batch_action.append([a])
            batch_reward.append([r])
            batch_rf_new.append(rf_prime)
            batch_cf_new.append(cf_prime)
            batch_action_new.append(action_prime)

        return torch.tensor(np.array(batch_rf), dtype=torch.float), torch.tensor(np.array(batch_cf), dtype=torch.float), torch.tensor(np.array(batch_action),dtype=torch.int64), torch.tensor(np.array(batch_reward),dtype=torch.int64),torch.tensor(np.array(batch_rf_new),dtype=torch.float), torch.tensor(np.array(batch_cf_new),dtype=torch.float),torch.tensor(np.array(batch_action_new),dtype=torch.int64)
        #torch.tensor(np.array(s_lst), dtype=torch.float), torch.tensor(np.array(a_lst),dtype=torch.int64), torch.tensor(np.array(r_lst),dtype=torch.int64),torch.tensor(np.array(s_prime_lst),dtype=torch.float)

class Agent:
    def __init__(self, action_set,epsilon):
        self.action_set = action_set
        self.action_number = len(action_set)
        self.epsilon = epsilon
        self.build_network()

    def build_network(self):
        self.Q_network = Model(self.action_number)
        self.target_network = Model(self.action_number)
        self.optimizer = optim.Adam(self.Q_network.parameters(), lr=Config.lr)

    def update_target_network(self):
        # copy current_network to target network
        self.target_network.load_state_dict(self.Q_network.state_dict())

    def update_Q_network(self, rf, cf, action, reward, rf_new, cf_new, action_new):
        #state = torch.from_numpy(state).float()/255.0
        #action = torch.from_numpy(action).float()
        #state_new = torch.from_numpy(state_new).float()/255.0
        #reward = torch.from_numpy(reward).float()



        state = []
        for i in range(32):
            input_batch = np.concatenate([rf[i], cf[i]])
            state.append(input_batch)
        state = torch.Tensor(state)

        state_new = []

        for i in range(32):
            input_batch_new = np.concatenate([rf_new[i], cf_new[i]])
            state_new.append(input_batch_new)
        state_new = torch.Tensor(state_new)
        action_new= action_new.view(-1,1)
        # use current network to evaluate action argmax_a' Q_current(s', a')_


        action_new_max = self.Q_network.forward(state_new,action_new).max(dim=1)[1].data.view(-1, 1)
        action_new_onehot = torch.zeros(Config.batch_size, self.action_number)
        action_new_onehot = action_new_onehot.gather(1,action_new_max)
        Q_target = (self.target_network.forward(state_new,action_new_max)*action_new_onehot).sum(dim=1)
        Q_target = Q_target.reshape(32,1)
        y = (reward + Config.discount_factor * Q_target)


        # use target network to evaluate value y = r + discount_factor * Q_tar(s', a')
        #y = (reward + Config.discount_factor * Q_target)

        Q = (self.Q_network.forward(state,action)).max(1)[0]
        Q=Q.reshape(32,1)
        #Q = (self.Q_network.forward(state,action)).gather(1,action)
        loss = F.smooth_l1_loss(Q, y)
        #loss = mse_loss(input=Q, target=y.detach()) #F.smooth_l1_loss(q_a, target)

        # backward optimize
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.data

    def take_action(self, rf, cf,action_num): #엡실론 사용해서 random뽑기 / 나머지 확률로 높은거 뽑기
        state = np.concatenate((rf, cf))
        state = torch.from_numpy(state).float()
        action = np.zeros(action_num)
        for i in range(0,action_num):
          action[i]= i
        action = torch.Tensor(action)
        q_list = []
        q_max_list = []
        idx=[]
        max= -1

        for i in range(0,action_num):
            out= self.Q_network.forward(state, action[i].view(1))
            q_list.append(out.item())
            idx.append(i)
        q_max_list = np.argsort(q_list)

        # with epsilon prob to choose random action else choose argmax Q estimate action
        if random.random() < self.epsilon:
          a,b,c = random.sample(idx,3)
          max = (q_list[a] + q_list[b] + q_list[c])/3
          return a,b,c,max
        else:
            max = (q_list[q_max_list[action_num-1]] + q_list[q_max_list[action_num-2]] + q_list[q_max_list[action_num-3]])/3
            return q_max_list[action_num-1], q_max_list[action_num-2],q_max_list[action_num-3], max

    def next_take_action(self,next_rf, next_cf,action_num):
        next_state = np.concatenate((next_rf, next_cf))
        next_state = torch.from_numpy(next_state).float()
        next_action = np.zeros(5)
        for i in range(0,5):
          next_action[i]= i
        next_action = torch.Tensor(next_action)
        q_list = []
        q_max_list = []
        for i in range(0,action_num):
          out= self.Q_network.forward(next_state, next_action[i].view(1))
          q_list.append(out.item())
        return np.argmax(q_list)


    def update_epsilon(self,n_epi):
        #if self.epsilon > Config.min_epsilon:
            #self.epsilon -= Config.epsilon_discount_rate

        if (self.epsilon > Config.min_epsilon) :
          epsilon = max(0, 0.9 - ((1 / Config.epochs) * n_epi))



def kakaoFeedback(cf,recommended_news1,recommended_news2,recommended_news3,user): #선호도 만들기
  favor_news=[1]*5

  if user==1:
    for j in range(0,5):
      if j==0:
        favor_news[j] = 8 #80%
      elif j==1:
        favor_news[j] = 8 #80%
      else:
        favor_news[j]= 2 #30%

  elif user==2:
    for j in range(0,5):
      if j==2:
        favor_news[j]= 8 #80%
      elif j==3:
        favor_news[j]= 8 #80%

      else:
        favor_news[j]= 2 #30%



  if  cf[3]==1:
    favor_news[4] = 10


  users_favor_news=[0]*5

  for i in range(0,5):
    k = random.randint(1,10)
    if(k<=favor_news[i]):
        users_favor_news[i]=1



  news_read=[-10,-10,-10]
  #뉴스 선호도 바탕으로 실제 뉴스가 들어왔을 때 뉴스를 읽게 만들기
  if users_favor_news[recommended_news1] ==1:
    news_read[0] =1
  if users_favor_news[recommended_news2]==1:
    news_read[1]=1
  if users_favor_news[recommended_news3]==1:
    news_read[2]=1
  return news_read

#평균내기 ,전체/개월전/한달전/일주일전 #next state만들기
def s_sp(n_epi,all):
  sum1=[0] * 5
  start1 = 0
  end1 = n_epi

  for i in range(start1, end1):
    for j in range(5):
      sum1[j]+=all[i][j]
  if(n_epi>0):
    for i in range(5):
      sum1[i]=sum1[i]/(end1-start1)

  sum2=[0] * 5
  start2=n_epi-30
  end2 = n_epi
  if start2<=0 :
    start2=0

  for i in range(start2,end2):
    for j in range(5):
      sum2[j]+=all[i][j]
  if(n_epi>0):
      for i in range(5):
        sum2[i]=sum2[i]/(end2-start2)


  sum3=[0] * 5
  start3=n_epi-14
  end3 = n_epi
  if start3<=0:
    start3=0

  for i in range(start3,end3):
    for j in range(5):
      sum3[j]+=all[i][j]
  if(n_epi>0):
      for i in range(5):
        sum3[i]=sum3[i]/(end3-start3)


  sum4=[0]*5
  for i in range(5):
    sum4[i]+=all[n_epi][i]



  # next State
  S_P= np.zeros(20)

  for i in range(5): #all/epoch
    S_P[i]= sum1[i]

  for i in range(5): #3개월
    S_P[5+i]=sum2[i]

  for i in range(5): #한달
    S_P[10+i]=sum3[i]

  for i in range(5): #일주일
    S_P[15+i]=sum4[i]


  return S_P

def load_users(epochs):
  users=[]
  users_rf=[]
  users_cf=[]
  all_list = []
  max_q_list = []
  reward_list = []

  for i in range(1,3):
    users.append(i)
    S=np.zeros(20)
    users_rf.append(S)
    all_list.append([])
    max_q_list.append([])
    reward_list.append([])
  for i in range(0,epochs+10):
    E=np.zeros(4)
    if i%4==0:
      E[0] =1
      users_cf.append(E)
    elif i%4==1:
      E[1] =1
      users_cf.append(E)
    elif i%4==2:
      E[2] =1
      users_cf.append(E)
    else:
      E[3] =1
      users_cf.append(E)

  return users,users_rf,users_cf,all_list,max_q_list, reward_list

def run(users, users_rf,users_cf, all_list, n_epi,answer,recommend,agent,buffer,avg_q_list, reward_list):
  n=len(users)
  action1=[0]*n
  action2 = [0] *n
  action3 = [0] * n

  isnewsread = [[0]*3 for _ in range(n)]
  avg_q_value = [0] * n
  rewards = [0] * n

#유저별로 추천하는 뉴스 만들기
  for user in range(n):
    rf = users_rf[user]
    cf = users_cf[n_epi]
    action1[user],action2[user], action3[user], avg_q_value[user] = agent.take_action(rf,cf, agent.action_number)
    avg_q_list[user].append(avg_q_value[user])


#유저별로 kakao톡에 추천 뉴스 보내주기
  for user in range(n):
    isnewsread[user] = kakaoFeedback(users_cf[n_epi], action1[user],action2[user],action3[user],users[user])

#유저별로 카카오톡에서 읽은 뉴스들 카테고리 가져와서 학습돌리기
  for user in range(n):
    a_user1_action1_reward= 0
    a_user1_action2_reward= 0
    a_user1_action3_reward = 0

    a_user = [0] * 5

    if isnewsread[user][0]>0:
      a_user1_action1_reward=100
      a_user[action1[user]] =1
      if(n_epi>=33):
        answer[user][action1[user]]+=1

      if  users_cf[n_epi][3]==1:
          if action1[user]==4:
              a_user1_action1_reward = 200


    if isnewsread[user][1]>0:
      a_user1_action2_reward=100
      a_user[action2[user]] =1
      if(n_epi>=33):
        answer[user][action2[user]]+=1
      if  users_cf[n_epi][3]==1:
           if action2[user]==4:
               a_user1_action2_reward = 200



    if isnewsread[user][2]>0:
      a_user1_action3_reward=100
      a_user[action3[user]] =1
      if(n_epi>=33):
        answer[user][action3[user]]+=1
      if users_cf[n_epi][3]==1:
           if action3[user]==4:
               a_user1_action3_reward = 200

    recommend[user][action1[user]]+=1
    recommend[user][action2[user]]+=1
    recommend[user][action3[user]]+=1


    all_list[user].append(a_user)

    S_P = s_sp(n_epi,all_list[user])

    rewards[user]+= (a_user1_action1_reward+ a_user1_action2_reward + a_user1_action3_reward)
    reward_list[user].append(rewards[user])


    if n_epi % Config.update_target_frequency == 0: #타겟넷 업데이트
      agent.update_target_network()

     # 버퍼에 저장
    next_cf = users_cf[n_epi+1]
    next_action= agent.next_take_action(S_P, next_cf, agent.action_number)

    buffer.append((users_rf[user],users_cf[n_epi],action1[user],a_user1_action1_reward,S_P,next_cf,next_action))
    buffer.append((users_rf[user],users_cf[n_epi],action2[user],a_user1_action2_reward,S_P,next_cf,next_action))
    buffer.append((users_rf[user], users_cf[n_epi], action3[user], a_user1_action3_reward, S_P, next_cf, next_action))
    if buffer.size() > 32:
      batch_rf, batch_cf, batch_action, batch_reward, batch_rf_new, batch_cf_new, batch_action_new = buffer.sample(Config.batch_size)
      loss= agent.update_Q_network(batch_rf, batch_cf, batch_action, batch_reward, batch_rf_new, batch_cf_new,batch_action_new)



    agent.update_epsilon(n_epi)

    #미니배치 학습, 경사하강법


    # state update
    users_rf[user]=S_P
    if(n_epi%1000==0):
        print("epoch = {} / user {}'s RECOMMEND: {}, {}, {}" .format(n_epi,users[user],action1[user],action2[user],action3[user]),end=" ")


    #Q-value 저장하기

  if(n_epi%1000==0):
      print('------------------------------------------------------------------------------')

  return users_rf, users_cf, all_list,answer,recommend, avg_q_list, reward_list

epochs = 50000
answer = []
recommend = []
for i in range(2):
  a=[0]*5
  answer.append(a)
  recommend.append(a)

epsilon = Config.initial_epsilon
agent= Agent(a,epsilon)
buffer= Replay_Buffer(Config.reply_buffer_limit)

users,users_rf,users_cf, all_list, avg_q_list, reward_list =load_users(epochs)


for n_epi in range(epochs):
  #epsilon = max(0, 1 - ((1 / epochs) * n_epi))
  #stop epsilon 필요
  users_rf,users_cf, all_list,answer,recommend, avg_q_list, reward_list =  run(users,users_rf,users_cf, all_list,n_epi,answer,recommend,agent,buffer,avg_q_list, reward_list)


path = 'ddqn_model_10000_21_save.pth'
torch.save(agent.Q_network.state_dict(), path)

with open ("all_list_100000_21.pkl","wb") as f:
    pickle.dump(all_list,f)


with open ("users_rf_10000_21.pkl","wb") as g:
    pickle.dump(users_rf,g)


for i in range(len(users)):
  print("user{} select:{}".format(users[i],answer[i]))
  print("user{} recommend:{}".format(users[i],recommend[i]))

user1_result = np.argsort(answer[0])
user2_result = np.argsort(answer[1])

print("user 1's most prefer news = {}, {}".format(user1_result[4],user1_result[3]))
print("user 2's most prefer news = {}, {}".format(user2_result[4],user2_result[3]))


import matplotlib.pyplot as plt
import numpy as np
category=['1','2','3','4','5']
for i in range(2):
  colors=['y'] * 5
  if i==0:
    colors[user1_result[4]]='C2'
    colors[user1_result[3]]='C2'
  elif i==1:
    colors[user2_result[4]]='C2'
    colors[user2_result[3]]='C2'




  n=np.arange(5)
  plt.bar(n,answer[i],color=colors)
  plt.xticks(n,category)
  plt.ylabel('Total Counts')
  plt.xlabel('Preferred Category Index')
  plt.show()
  plt.savefig("Test Graph{}.png".format(i+1))

  pd.Series(reward_list[i]).to_csv('Reward_User_{}.csv'.format(i+1))
  pd.Series(avg_q_list[i]).to_csv('Avg_q_list_User_{}.csv'.format(i+1))

reward_user1 = pd.read_csv('Reward_User_1.csv',index_col=False).iloc[:,1]
reward_user2 = pd.read_csv('Reward_User_2.csv',index_col=False).iloc[:,1]


q_user1 = pd.read_csv('Avg_q_list_User_1.csv',index_col=False).iloc[:,1]
q_user2 = pd.read_csv('Avg_q_list_User_2.csv',index_col=False).iloc[:,1]


plt.figure(figsize=(16,8))
plt.plot(reward_user1.cumsum() / (pd.Series(np.arange(reward_user1.shape[0]))+1), label = "Reward_User1")
plt.plot(reward_user2.cumsum() / (pd.Series(np.arange(reward_user2.shape[0]))+1), label = "Reward_User2")

plt.legend()
plt.show()

plt.figure(figsize=(16,8))
plt.plot(q_user1.cumsum() / (pd.Series(np.arange(q_user1.shape[0]))+1), label = "Q_value_User1")
plt.plot(q_user2.cumsum() / (pd.Series(np.arange(q_user2.shape[0]))+1), label = "Q_value_User2")

plt.legend()
plt.show()


